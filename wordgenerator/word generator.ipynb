{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94105c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullanilan Cihaz: cuda\n",
      "\n",
      "Toplam kelime: 39\n",
      "Benzersiz karakter: 25\n",
      "Vocabulary boyutu: 28\n",
      "Karakterler: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', 'z', 'ö', 'ü', 'ş']\n",
      "\n",
      "Model parametreleri: 602,012\n",
      "\n",
      "==================================================\n",
      "Egitim Basldai... Device: cuda\n",
      "==================================================\n",
      "\n",
      "Epoch 10/100 - Loss: 1.4935\n",
      "ornek Kelimeler: ['eynin', 'eyi', 'şesra']\n",
      "\n",
      "Epoch 20/100 - Loss: 0.9527\n",
      "ornek Kelimeler: ['met', 'barak', 'alp']\n",
      "\n",
      "Epoch 30/100 - Loss: 0.8752\n",
      "ornek Kelimeler: ['kerem', 'ege', 'burahüse']\n",
      "\n",
      "Epoch 40/100 - Loss: 0.8236\n",
      "ornek Kelimeler: ['burak', 'ada', 'arda']\n",
      "\n",
      "Epoch 50/100 - Loss: 0.7463\n",
      "ornek Kelimeler: ['ömer', 'mehmet', 'burak']\n",
      "\n",
      "Epoch 60/100 - Loss: 0.7641\n",
      "ornek Kelimeler: ['ömer', 'selin', 'zeynep']\n",
      "\n",
      "Epoch 70/100 - Loss: 0.7356\n",
      "ornek Kelimeler: ['mert', 'ece', 'cem']\n",
      "\n",
      "Epoch 80/100 - Loss: 0.7337\n",
      "ornek Kelimeler: ['arda', 'deniz', 'selin']\n",
      "\n",
      "Epoch 90/100 - Loss: 0.7163\n",
      "ornek Kelimeler: ['veli', 'emre', 'mert']\n",
      "\n",
      "Epoch 100/100 - Loss: 0.7275\n",
      "ornek Kelimeler: ['ömer', 'ahmet', 'selin']\n",
      "\n",
      "Egitim Tamamlandi\n",
      "\n",
      "==================================================\n",
      "YENI KELIMELER URETILIYOR...\n",
      "==================================================\n",
      "\n",
      "Set 1: mehmet, hasan, fatma, selim, selim, dömefne, defne, bran, seda, ömer, onur, ege, hasan, mert, seda, hasan, esra, mehmet, ahmet, alp, mert, ehmet, kerem, burak, kan, deniz, eren, doruk, kan, buse, doruk, ada, zeynep, buse, hasan, berk, mert, ömer, ömer, pelin, ege, yusuf, ahmet, selim, berk, ahmet, selim, kaama, kan, onur, doruk, can, zeynep, cem, fatma, berk, buse, eren, berk, fatma, burak, doruk, berk, deniz, ömer, deniz, ece, hasan, pelin, mert, hasan, ibrahim, defne, ayşe, can, ahmet, mustafa, ege, selim, cem, selim, hürap, eren, fatma, ege, can, mehmet, alp, kan, doruk, hasan, kan, selin, kerem, elif, ibrahm, burak, doruk, berk, seda\n",
      "Set 2: mehmet, burak, ege, buran, ece, alp, fatma, e, buse, ege, mustafa, eren, ayşe, emre, berk, eren, ömer, selin, kerem, ahmet, alp, defne, mehmet, doruk, arda, eren, mert, onur, kan, emre, ege, ege, alp, onur, arda, ege, kem, yusuf, selin, baran, pelin, burak, eren, eren, berk, mert, arda, ece, ge, zeynep, buse, ece, mert, mert, mert, ömer, ege, yusuf, berk, selin, kerem, zeynep, ahmet, fatma, berk, mert, berk, esra, ömer, ayşe, cem, selin, zeynep, kerem, ece, kan, onur, ege, fatma, berk, ömer, hasan, selim, seda, buse, can, kerem, deniz, veli, berk, zeynep, doruk, veli, mehmet, defne, mert, berk, berk, hüseyin, berk\n",
      "Set 3: alp, mustafa, yusuf, burak, defne, ibrahim, kerem, hasan, kan, hasan, cem, seda, ece, alp, pelin, onur, elif, seda, ibrahim, defne, hasan, esra, buse, can, burak, kan, ege, burak, ege, kerem, eren, berk, emre, ibrahim, buse, yusuf, fatma, doruk, eren, defa, selin, mert, selim, ege, defne, veli, berk, zeynep, baran, arda, berk, hüseyin, mert, kan, pelin, aada, ömer, fatma, ibrarahim, berk, burak, pelin, selin, kan, burak, onur, eren, yusuf, veli, aruk, ömer, eren, ada, eren, alp, doruk, ada, ege, selim, fatma, esra, can, ege, mert, pelin, arda, fatma, selin, yusuf, ece, onur, defne, kerem, berk, doruk, pelin, mert, elif, balp, hüseyin\n",
      "Set 4: eren, kan, ece, kan, emre, ibrahim, mert, alp, doruk, eren, ayşe, eren, buse, arda, ali, veli, selin, ada, ada, defne, cem, mert, selim, selim, onur, arda, kerem, baran, berk, cem, yusuf, arda, ömer, berk, mert, emre, onur, yusuf, ibrahim, mert, eren, ege, alp, onur, ibrahim, burak, selim, deniz, ayşe, hüseyin, emre, yusuf, kan, ege, deniz, selim, can, ömer, zeynep, ömer, kerem, can, kan, eren, kan, veli, defne, elif, ömer, buse, fatma, hüömet, ibrahim, ayşe, burak, burak, ali, ayşe, ömer, onur, hüseyin, arda, hüseyin, emre, zeynep, ada, hasan, mert, ömer, ada, ayşe, zeynep, seda, fatma, hüseyin, burak, ibrahim, selin, alp, mert\n",
      "Set 5: ayşe, elif, eren, ada, esra, deoreniz, seda, veli, eren, deniz, doruk, zeynep, ömer, berk, mehmet, ece, ege, fatma, ece, berk, elif, ege, alp, yusep, kan, yusuf, esra, arda, cem, onur, hüseyin, berk, onur, selim, mehmet, defne, deniz, ömer, onur, defne, ege, mert, eren, can, berk, yusuf, ahmet, pelin, mehmet, berk, ada, mert, alp, kerem, ege, can, hüseyin, ece, veli, berk, kan, kan, se, ahmet, veli, onur, mehmet, ayşe, doruk, kan, eren, ibuse, seda, burak, alp, baran, deniz, arim, ömer, iece, ayşe, onur, ahmet, hasan, berk, elif, hüseyin, burak, ahmet, ibrahim, ege, kan, defne, ege, onur, deniz, emre, selin, berk, doruk\n",
      "Set 6: ali, can, eren, eren, ran, doruk, fatma, berk, pelin, esra, berk, fatmk, ada, burak, ece, ayşe, hüseyin, hüseyin, kan, burak, burak, eren, zeynep, burak, mustafa, ali, berk, ece, ali, mert, honuseyn, alp, pelin, ayşe, ömer, fatma, hüsan, ahmet, ömer, ayşe, ayşe, zeynep, alp, can, kerem, berk, kan, yusuf, yusuf, hasan, yusuf, hüseyin, buse, mustafa, alp, berk, alp, esra, kerem, ece, berk, defne, eren, defne, esra, seda, emre, mehmet, burak, mehmet, can, kerem, berk, hamet, yusuf, ece, alp, ada, burak, eren, mustafa, mehmet, veli, onur, veli, berk, zeynep, erk, seda, seda, onur, ömer, onur, pelin, berk, ece, pelin, doruk, yusuf, eren\n",
      "Set 7: doruk, burak, selim, ege, kerem, mert, alp, mert, baran, ege, can, kerem, mert, kerem, ece, zeynek, esra, ayşe, eren, hüseyin, baran, ayşe, deniz, onur, ömer, berk, seda, pelin, alp, selim, eren, deniz, mehmet, onur, deniz, burak, ahmet, zeynep, defne, eren, yusuf, ece, kburak, defne, mert, doruk, zeynep, emre, ahmet, kan, seda, kerem, burak, kan, alp, berk, baran, ege, ers, elif, eren, ömer, ömer, ada, fatma, ahmet, zeynep, doruk, yusuf, hasan, alp, hüseyin, berk, ayşe, emre, berk, ahmet, eren, ada, mert, emre, defne, ömer, ahmet, arda, ali, alp, hasan, alp, defne, onur, seda, pelin, elif, hüseyin, ada, pelin, emre, cem, burak\n",
      "Set 8: buse, arda, onur, hüseyin, kerem, seda, mert, mehmet, usea, ege, eren, berk, buse, ada, eren, ali, kan, burak, hüseyin, selim, yusuf, mert, eren, cem, hasan, berk, mert, fatma, emre, ahmet, ece, zeynep, veli, ege, doruk, selin, seda, deniz, ada, elif, can, onur, ahmet, defne, yusuf, ibrahim, cem, ece, pelin, ege, kan, mert, alp, selin, defne, kan, cem, can, baran, ayşe, eren, onur, mert, mustafa, aly, selin, ömer, burak, emre, kan, elif, ömer, hüseyin, ayşe, baran, hest, kerem, yusuf, selim, esra, mehmet, mahmet, berk, berk, deniz, ege, burak, hüseyin, kerem, mert, meyin, esra, ali, emore, fatma, hasan, edef, ada, pelin, selin\n",
      "Set 9: ece, selim, berk, kan, defne, defne, burak, arda, esra, emre, veli, hüseyin, ali, ayşe, ömer, ibrahim, burak, mert, seda, doruk, ayşe, emre, eren, mustafa, mahmet, defne, cem, veli, doruk, lp, ahmet, defne, dorue, mehmet, kan, veli, ibrahim, doruk, ahmet, buse, ece, ali, zeynep, hüseyin, kan, selim, ibrahfn, ege, alp, esra, onur, mehmet, esra, arda, alp, berk, ayşe, hasan, defne, arda, burak, selin, baran, eren, bumerk, deföm, zeynep, ece, ada, pelin, ece, fatma, selin, veli, veli, ali, ada, onur, can, emre, onur, defne, burak, ayşe, hüseyin, ayşe, hüseyin, ibrahim, veli, seda, mert, ada, defne, veli, defne, ada, mert, ahmet, zeynep, kan\n",
      "Set 10: buse, mehmet, defne, doruk, alp, ege, pelin, nert, burak, defne, fatma, mert, defne, ömer, onur, ömer, ayşe, elif, veli, doruk, ömer, alp, eren, eren, defne, emre, veli, elif, eren, burak, baran, deniz, fatma, ayşe, defne, ayşe, fatma, ayşe, hüseyin, doruk, mustafa, hasan, ayşe, alp, burak, emre, ali, fatma, arda, elif, kan, emre, can, emre, zeynep, ece, onur, ege, onur, defne, onur, ege, veli, hüseyin, mehmet, selim, defne, ahmet, mehmet, defne, arda, yusuf, ayşe, defne, ayşe, berk, mert, baran, onur, defne, ada, buse, ahmet, ayşe, buse, cem, mert, ayşe, pelin, ahmet, kurak, veli, can, emre, ran, ahmet, meort, alp, ece, pelin\n",
      "\\Bitti !\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attn_output, _ = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attn_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class WordGenerator(nn.Module):\n",
    "    \"\"\"Kelime üreten decoder modeli\"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, \n",
    "                 num_layers=3, d_ff=512, dropout=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
    "        return ~mask\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if mask is None:\n",
    "            seq_len = x.size(1)\n",
    "            mask = self.generate_square_subsequent_mask(seq_len, x.device)\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        output = self.fc_out(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class WordDataset:\n",
    "    \"\"\"kelimleri alir ve vocabulary oluturur\"\"\"\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        \n",
    "        chars = set(''.join(words)) #kullanilan tum karakterleri bul ve sozluge ekle\n",
    "        self.chars = sorted(list(chars))#karakterleri sirala ve properties olarak kaydet\n",
    "        \n",
    "        # Ozel tokenlari ekle padding start ve end\n",
    "        self.char_to_idx = {'<PAD>': 0, '<START>': 1, '<END>': 2}\n",
    "        for i, c in enumerate(self.chars):\n",
    "            self.char_to_idx[c] = i + 3 #her bir karakteri charto idxe ekle ve her karaktere sayisa bir deger ekle a:3 b:4 gibi\n",
    "        \n",
    "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()} #key value lari tesrcevir\n",
    "        self.vocab_size = len(self.char_to_idx) #vocabulerity size hesap eder\n",
    "        \n",
    "        print(f\"Toplam kelime: {len(words)}\")\n",
    "        print(f\"Benzersiz karakter: {len(self.chars)}\")\n",
    "        print(f\"Vocabulary boyutu: {self.vocab_size}\")\n",
    "        print(f\"Karakterler: {self.chars}\")\n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \"\"\"Kelimeyi index numaralrina gore sayisallastariri\"\"\"\n",
    "        return [self.char_to_idx['<START>']] + \\\n",
    "               [self.char_to_idx[c] for c in word] + \\\n",
    "               [self.char_to_idx['<END>']]\n",
    "    \n",
    "    def decode_indices(self, indices):\n",
    "        \"\"\"index listesini kelimeye cevirir\"\"\"\n",
    "        chars = []\n",
    "        for idx in indices:\n",
    "            if idx == self.char_to_idx['<END>']:\n",
    "                break\n",
    "            if idx != self.char_to_idx['<START>'] and idx != self.char_to_idx['<PAD>']:\n",
    "                chars.append(self.idx_to_char[idx])\n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def create_batches(self, batch_size=32):\n",
    "        \"\"\"egitim batchleri olusturur her bit tensoru ayni boyuta getirir\"\"\"\n",
    "        random.shuffle(self.words)\n",
    "        batches = []\n",
    "        \n",
    "        for i in range(0, len(self.words), batch_size):\n",
    "            batch_words = self.words[i:i+batch_size]\n",
    "            \n",
    "            # encode\n",
    "            encoded = [self.encode_word(w) for w in batch_words]\n",
    "            \n",
    "            # padding\n",
    "            max_len = max(len(seq) for seq in encoded)\n",
    "            padded = []\n",
    "            for seq in encoded:\n",
    "                padded.append(seq + [self.char_to_idx['<PAD>']] * (max_len - len(seq)))\n",
    "            \n",
    "            # input target\n",
    "            input_seqs = [seq[:-1] for seq in padded]  # Son harf haric\n",
    "            target_seqs = [seq[1:] for seq in padded]  # İlk harf haric\n",
    "            \n",
    "            batches.append((\n",
    "                torch.tensor(input_seqs, dtype=torch.long),\n",
    "                torch.tensor(target_seqs, dtype=torch.long)\n",
    "            ))\n",
    "        \n",
    "        return batches\n",
    "\n",
    "\n",
    "def train_model(model, dataset, device, epochs=50, batch_size=32, lr=0.001):\n",
    "    \"\"\"Meodeli Egitir\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.char_to_idx['<PAD>'])\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Egitim Basldai... Device: {device}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batches = dataset.create_batches(batch_size)\n",
    "        \n",
    "        for input_seq, target_seq in batches:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(input_seq)\n",
    "            \n",
    "            # Loss hesapla\n",
    "            loss = criterion(output.view(-1, model.vocab_size), target_seq.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(batches)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "            # ornek kelime uret\n",
    "            sample_words = generate_words(model, dataset, device, num_words=3, max_len=15)\n",
    "            print(f\"ornek Kelimeler: {sample_words}\\n\")\n",
    "    \n",
    "    print(\"Egitim Tamamlandi\\n\")\n",
    "\n",
    "\n",
    "def generate_words(model, dataset, device, num_words=5, max_len=20, temperature=1.0):\n",
    "    '''Kelimler uretir eval modunda egitim yapmaz gradien duzenleme yapmaz calisma modu'''\n",
    "    model.eval()\n",
    "    generated_words = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_words):\n",
    "            # <START> token baslar\n",
    "            current_seq = [dataset.char_to_idx['<START>']]\n",
    "            \n",
    "            for _ in range(max_len):\n",
    "                input_tensor = torch.tensor([current_seq], dtype=torch.long).to(device)\n",
    "                output = model(input_tensor)\n",
    "                \n",
    "                # son karakterlerin olasiliklari\n",
    "                logits = output[0, -1, :] / temperature\n",
    "                probs = F.softmax(logits, dim=0)\n",
    "                \n",
    "                # ornekleme yap\n",
    "                next_idx = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                # <END> veya <PAD> gelirse dur\n",
    "                if next_idx == dataset.char_to_idx['<END>'] or \\\n",
    "                   next_idx == dataset.char_to_idx['<PAD>']:\n",
    "                    break\n",
    "                \n",
    "                current_seq.append(next_idx)\n",
    "            \n",
    "            # decode et\n",
    "            word = dataset.decode_indices(current_seq)\n",
    "            if word:  # bos degilse ekle\n",
    "                generated_words.append(word)\n",
    "    \n",
    "    return generated_words\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ornek Kelime isimler isim secilmesinin nedeni fakli dil ailelerine mensup olmalari ve farkli frekans sikligi olmasi bunu ogrenirse herseyi ogrenir\n",
    "    words = [\n",
    "        \"ahmet\", \"mehmet\", \"ali\", \"veli\", \"ayşe\", \"fatma\", \"zeynep\", \"elif\",\n",
    "        \"mustafa\", \"ibrahim\", \"hasan\", \"hüseyin\", \"emre\", \"cem\", \"can\",\n",
    "        \"deniz\", \"ege\", \"mert\", \"yusuf\", \"ömer\", \"selim\", \"kerem\", \"berk\",\n",
    "        \"ada\", \"ece\", \"selin\", \"defne\", \"buse\", \"esra\", \"seda\", \"pelin\",\n",
    "        \"burak\", \"onur\", \"kaan\", \"baran\", \"eren\", \"arda\", \"alp\", \"doruk\"\n",
    "    ]\n",
    "    \n",
    "    # Device sec\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Kullanilan Cihaz: {device}\\n\")\n",
    "    \n",
    "    # Dataset olustur\n",
    "    dataset = WordDataset(words)\n",
    "    \n",
    "    # Model olustur\n",
    "    model = WordGenerator(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        d_model=128,\n",
    "        num_heads=4,\n",
    "        num_layers=3,\n",
    "        d_ff=512,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel parametreleri: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Modeli egit\n",
    "    train_model(model, dataset, device, epochs=100, batch_size=16, lr=0.001)\n",
    "    \n",
    "    # Yeni kelimeler uret\n",
    "    print(f\"{'='*50}\")\n",
    "    print(\"YENI KELIMELER URETILIYOR...\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    for i in range(10):\n",
    "        new_words = generate_words(model, dataset, device, num_words=100, max_len=15)\n",
    "        print(f\"Set {i+1}: {', '.join(new_words)}\")\n",
    "    \n",
    "    print(\"\\Bitti !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b291d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
